{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering & Geodemographics\n",
    "\n",
    "A common challenge in data analysis is how to group observations in a data set together in a way that allows for generalisation: _this_ group of observations are similar to one another, _that_ group is dissimilar to this group. But what defines similarity and difference? There is no _one_ answer to that question and so there are many different ways to cluster data, each of which has strengths and weaknesses that make them more, or less, appropriate in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red;\">Important Note for Mac Users</span>\n",
    "\n",
    "Recent changes in the way that the Mac OS handles the plotting of data means that you need to do certain things in a specific order at the start of any notebook in which you intend to show maps or graphs. Please make a copy of the following code for any notebook that you create and make it the _first_ code that you run in the notebook:\n",
    "\n",
    "```python\n",
    "# Needed on a Mac\n",
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "%matplotlib inline\n",
    "```\n",
    "\n",
    "For non-Mac users it _should_ be:\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "```\n",
    "\n",
    "This _should_ enable you to create plots, including in the practical that we're about to start! If you forget to run this code then you will probably need to restart the Kernel (Kernel > Restart from the menu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Organised\n",
    "\n",
    "To get started we're going to work with pandas and geopandas -- again, nothing new so far but you'll see we've got some new libraries here.\n",
    "\n",
    "### Specifying the Kernel\n",
    "\n",
    "**_Note:_** Before you go any further, we need to check that you've got the right 'Kernel' (virutal environment) specified in Jupyter. At top right it should say \"Python [spats]\" or \"Python [gsa2017]\" (or something very similar to one of those!) and that is the environment that we want to work in: spats is short Spatial Analysis and that contains all of the libraries that we need for our research. There are other kernels configured and these can be accessed by clicking on the 'Kernel' menu item and then 'Change Kernel'. This feature is well beyond the scope of this practical, but it basically allows you to run multiple 'versions' of Python with different libraries or versions of libraries installed at the same time.\n",
    "\n",
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysal as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import requests\n",
    "import zipfile\n",
    "import re\n",
    "import os\n",
    "import pickle as pk\n",
    "from sklearn import cluster\n",
    "from io import BytesIO, StringIO\n",
    "from os.path import join as pj\n",
    "from pathlib import Path\n",
    "\n",
    "import sklearn\n",
    "sklv = int(sklearn.__version__.replace(\".\",\"\"))\n",
    "if sklv < 180:\n",
    "    print(\"SciKit-Learn verion is: \" + sklearn.__version__)\n",
    "    print(\"This notebook relies on a version > 0.18.0\")\n",
    "\n",
    "import random\n",
    "random.seed(123456789) # For reproducibility\n",
    "\n",
    "# Make numeric display a bit easier\n",
    "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering in Python\n",
    "\n",
    "The most commonly-used _aspatial_ clustering algorighms are all found in [scikit-learn](http://scikit-learn.org/stable/), so that will be the focus of this practical. But just as there are aspatial and spatial statistics, there are also _spatially-aware_ clustering algorithms to be found in [PySAL](http://pysal.readthedocs.io/en/latest/), the Python Spatial Analysis Library.\n",
    "\n",
    "### Clustering in sklearn\n",
    "\n",
    "One organisation recently produced a handy scikit-learn cheatsheet that you should [download](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf). The terminology used in scikit-learn is rather different from anything you will have encountered before (unless you've studied computer science and, possibly, statistics) so it's worth spending a few minutes mapping what you already know on to the sklearn framework:\n",
    "\n",
    "  | Continuous | Categorical\n",
    "- | ---------- | -----------\n",
    "**Supervised** | Regression | Classification\n",
    "**Unsupervised** | Dimensionality Reduction | Clustering\n",
    "\n",
    "So clustering is a form of unsupervised (because we don't train the model on what a 'good' result looks like) and categorical (because we get labels out of the model, not predictors) machine learning. Clustering is often used together with PCI (Principal Components Analysis) which is a form of unsupervised dimensionality reduction: data sets with \"high dimensionality\" are reduced using PCI (you can think of this as a realignment of the axes with the 'data cloud') which has the effect of _maximising the variance_ on each new axis, and the reduced-dimension dataset is then fed to a clustering algorithm. Similarly, supervised approaches are often paired: logistic regression (supervised) is often used with classification (supervised).\n",
    "\n",
    "Anyway, here's a map to sklearn's algorithms and how to navigate them:\n",
    "\n",
    "<a href=\"http://scikit-learn.org/stable/tutorial/machine_learning_map/\"><img alt=\"SciKit-Learn Algorithm Map\" src=\"http://scikit-learn.org/stable/_static/ml_map.png\"></a>\n",
    "\n",
    "### Clustering in PySAL\n",
    "\n",
    "PySAL is similarly complex and _also_ has a map to help you navigate its complexities -- in this case we're particularly interested in the orange 'branch' of PySAL (labelled clustering!):\n",
    "\n",
    "![PySAL Map](http://darribas.org/gds_scipy16/content/figs/pysal.png)\n",
    "\n",
    "### Which Approach is Right?\n",
    "\n",
    "The reason that there is no 'right' approach (as I said above) is that it all depends on what you're trying to accomplish and how you're _reasoning_ about your problem. The image below highlights the extent to which the different clustering approaches in sklearn can produce different results -- and this is only for the _non-geographic_ algorithms!\n",
    "\n",
    "<a href=\"http://scikit-learn.org/stable/modules/clustering.html#clustering\"><img alt=\"Clustering algorithm comparison\" src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png\" /></a>\n",
    "\n",
    "To think about this in a little more detail:\n",
    "\n",
    "* If I run an online company and I want to classify my customers on the basis of their product purchases, then I probably don't care much about where they are, only about what they buy. So my clustering approach doesn't need to take geography into account. I might well _discover_ that many of my most valuable customers live in a few areas, but that is a finding, not a factor, in my research.\n",
    "* Conversely, if I am looking for cancer clusters then I might well care a _lot_ about geography because I want to make sure that I don't overlook a bigger cluster because it's 'hidden' inside an area with lots of non-sufferers. In that case, I want my clusters to take geography into account because I'm looking for agglomerations. That approach might classify an area with a smaller proportion of cancer patients as part of a 'cancer cluster' but that's because it is still significant _because_ of the geography.\n",
    "\n",
    "So you can undertake a spatial analysis using _either_ approach, it just depends on the role that you think geography should play in producing the clusters in the first place. We'll see this in action today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Geodemographic Data\n",
    "\n",
    "For the sake of simplicity we're going to work with roughly the same set of data for London that Alexiou & Singleton used in their _Geodemographic Analysis_ chapter from [Geocomputation: A Practical Primer](https://uk.sagepub.com/en-gb/eur/geocomputation/book241023). Although the implementation in the _Primer_ is in the R programming language, the concerns and the approach are exactly the same.\n",
    "\n",
    "### NomisWeb\n",
    "\n",
    "In case you've forgotten, nearly the _entire_ Census is available to download from [InFuse](http://infuse2011.ukdataservice.ac.uk/), but you can often download data 'in bulk' from [NomisWeb](https://www.nomisweb.co.uk/home/detailedstats.asp?resume=no) directly. \n",
    "\n",
    "#### Demographic Data\n",
    "\n",
    "The tables we want are:\n",
    "* KS102EW: Age structure\n",
    "* KS201EW: Ethnic group\n",
    "* KS401EW: Dwellings, household space and accommodation type\n",
    "* KS402EW: Tenure\n",
    "* KS403EW: Rooms, bedrooms and central heating\n",
    "* KS404EW: Car or van availability\n",
    "* KS501EW: Qualifications and students\n",
    "* KS603EW: Economic Activity by Sex\n",
    "\n",
    "#### Geographic Areas\n",
    "\n",
    "We want London LSOAs, which you can get by specifying 'Select areas within', then '2011 - super output areas - lower layers', and 'region' (leading to London). \n",
    "\n",
    "#### Saving Time\n",
    "\n",
    "To save you the trouble of manually selecting and downloading each table I have assembled everything into a '[Census.zip](https://github.com/kingsgeocomp/applied_gsa/blob/master/data/Census.zip?raw=true)' file. This will be automatically downloaded into a directory called `data` using the code below and you do _not_ need to unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'https://github.com/kingsgeocomp/applied_gsa/blob/master/data/Census.zip?raw=true'\n",
    "dst = 'analysis/Census.zip'\n",
    "\n",
    "if not os.path.exists(dst):\n",
    "    if not os.path.exists(os.path.dirname(dst)):\n",
    "        os.makedirs(os.path.dirname(dst))\n",
    "    \n",
    "    r = requests.get(src, stream=True)\n",
    "    \n",
    "    with open(dst, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=128):\n",
    "            fd.write(chunk)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONS Boundary Data\n",
    "\n",
    "We also need to download the LSOA boundary data. A quick Google search on \"2011 LSOA boundaries\" will lead you to the [Data.gov.uk portal](https://data.gov.uk/dataset/lower_layer_super_output_area_lsoa_boundaries). The rest is fairly straightforward:\n",
    "* We want 'generalised' because that means that they've removed some of the detail from the boundaries so the file will load (and render) more quickly.\n",
    "* We want 'clipped' because that means that the boundaries have been clipped to the edges of the land (e.g. the Thames; the 'Full' data set splits the Thames down the middle between adjacent LSOAs).\n",
    "\n",
    "**_Note:_** be sure that you select the **2011** data, not the **2001** data.\n",
    "\n",
    "#### Saving Time\n",
    "\n",
    "Again, in order to get you started more quickly I've already created a 'pack' for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'https://github.com/kingsgeocomp/applied_gsa/blob/master/data/Lower_Layer_Super_Output_Areas_December_2011_Generalised_Clipped__Boundaries_in_England_and_Wales.zip?raw=true'\n",
    "dst = 'analysis/LSOAs.zip'\n",
    "zpd = 'analysis/'\n",
    "\n",
    "if not os.path.exists(dst):\n",
    "    if not os.path.exists(os.path.dirname(dst)):\n",
    "        os.makedirs(os.path.dirname(dst))\n",
    "    \n",
    "    r = requests.get(src, stream=True)\n",
    "    \n",
    "    with open(dst, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=128):\n",
    "            fd.write(chunk)\n",
    "\n",
    "if not os.path.exists(zpd):\n",
    "    os.makedirs(os.path.dirname(zpd))\n",
    "    \n",
    "zp = zipfile.ZipFile(dst, 'r')\n",
    "zp.extractall(zpd)\n",
    "zp.close()    \n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "If you _haven't_ used the automated download code above, then you should drag both the LSOA boundary file and the Census zipfile into a 'data' directory that is the same directory as this notebook so that they're easy to access. You should then:\n",
    "* Unzip _only_ the LSOA zipfile.\n",
    "* Rename the directory containing LSOA data to 'lsoa'.\n",
    "\n",
    "And we're ready to go!\n",
    "\n",
    "### Other Sources of Data\n",
    "\n",
    "If you're more interested in US Census data then there's a nice-looking (I haven't used it) [wrapper to the Census API](https://pypi.python.org/pypi/census)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Geodata\n",
    "\n",
    "Unlike the NS-SeC data this is fairly straightforward using geopandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(os.path.join('analysis','lsoas','Lower_Layer_Super_Output_Areas_December_2011_Generalised_Clipped__Boundaries_in_England_and_Wales.shp'))\n",
    "gdf.crs = {'init' :'epsg:27700'}\n",
    "print(\"Shape of LSOA file: {0} rows by {1} columns\".format(gdf.shape[0], gdf.shape[1]))\n",
    "gdf.set_index('lsoa11cd', drop=True, inplace=True)\n",
    "gdf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Census Data\n",
    "\n",
    "You may need to make a few adjustments to the path to get the data loaded on your own computer. But notice what we're now able to do here: using the `zipfile` library we can extract a data file (or any other file) from the Zip archive without even having to open it. Saves even more time _and_ disk space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = zipfile.ZipFile(os.path.join('analysis','Census.zip'))\n",
    "z.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to save each data set to a\n",
    "# separate data frame to make it easier\n",
    "# to work with during cleaning\n",
    "d = {}\n",
    "\n",
    "for r in range(0, len(z.namelist())):\n",
    "    \n",
    "    m  = re.search(\"(?:-)([^\\.]+)\", z.namelist()[r])\n",
    "    nm = m.group(1)\n",
    "    \n",
    "    print(\"Processing {0} file: \".format(nm))\n",
    "    \n",
    "    with z.open(z.namelist()[r]) as f:\n",
    "                \n",
    "        if z.namelist()[r] == '99521530-Activity.csv': \n",
    "            d[nm] = pd.read_csv(BytesIO(f.read()), header=7, skip_blank_lines=True, skipfooter=7, engine='python')\n",
    "        else:\n",
    "            d[nm] = pd.read_csv(BytesIO(f.read()), header=6, skip_blank_lines=True, skipfooter=7, engine='python')\n",
    "    \n",
    "    print(\"\\tShape of dataframe is {0} rows by {1} columns\".format(d[nm].shape[0], d[nm].shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying Up\n",
    "\n",
    "OK, that's a _lot_ of data! 4,835 rows and 88 columns. However, we don't know how much of this is redundant and so need to work out what might need removing from the data set before we can try clustering. So we're going to work our way through each data set in turn so that we can convert them to percentages before combining them into a single, large data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dwellings\n",
    "\n",
    "From dewllings we're mainly interested in the housing type since we would expect that housing typologies will be a determinant of the types of people who live in an area. We _could_ look at places with no usual residents as well, or explore the distribution of shared dwellings, but this is a pretty good start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'Dwellings'\n",
    "\n",
    "# If we find this column, this deletes it\n",
    "if np.where(d[t].columns.values=='2011 super output area - lower layer')[0] >= 0:\n",
    "    d[t] = d[t].drop('2011 super output area - lower layer', 1)\n",
    "\n",
    "# List all of the columns    \n",
    "d[t].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns we're interested in analysing\n",
    "selection = [u'Whole house or bungalow: Detached', u'Whole house or bungalow: Semi-detached',\n",
    "       u'Whole house or bungalow: Terraced (including end-terrace)',\n",
    "       u'Flat, maisonette or apartment: Purpose-built block of flats or tenement',\n",
    "       u'Flat, maisonette or apartment: Part of a converted or shared house (including bed-sits)',\n",
    "       u'Flat, maisonette or apartment: In a commercial building',\n",
    "       u'Caravan or other mobile or temporary structure']\n",
    "\n",
    "# We can't be sure how the totals add up \n",
    "# so it's best to generate our own based\n",
    "# on the selection\n",
    "d[t]['Total Properties'] = d[t].loc[:, selection].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame to \n",
    "# hold the percentage values\n",
    "# and initialise it with only\n",
    "# the 'mnemonic' (i.e. GeoCode)\n",
    "d_pct = pd.concat(\n",
    "    [d[t]['mnemonic']], \n",
    "    axis=1, \n",
    "    keys=['mnemonic'])\n",
    "\n",
    "# For each of the columns remaining\n",
    "# in the select\n",
    "for c in selection:\n",
    "    m  = re.search(\"^(?:[^\\:]*)(?:\\:\\s)?(.+)$\", c)\n",
    "    nm = m.group(1)\n",
    "    if nm == 'e':\n",
    "        nm = 'Caravan'\n",
    "    print(\"Renaming \" + c + \" to \" + nm)\n",
    "    d_pct[nm] = pd.Series(d[t][c].astype(float)/d[t]['Total Properties'].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[t + '_pct'] = d_pct\n",
    "d[t + '_pct'].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age\n",
    "\n",
    "Clearly, some areas have more young people, some have older people, and some will be composed of families. A lot of these are going to be tied to 'lifestage' and so will help us to understand something about the types of areas in which they live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'Age'\n",
    "\n",
    "if np.where(d[t].columns.values=='2011 super output area - lower layer')[0] >= 0:\n",
    "    d[t] = d[t].drop('2011 super output area - lower layer', 1)\n",
    "\n",
    "d[t].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived columns\n",
    "d[t]['Age 5 to 14']  = d[t]['Age 5 to 7'] + d[t]['Age 8 to 9'] + d[t]['Age 10 to 14'] \n",
    "d[t]['Age 15 to 24'] = d[t]['Age 15'] + d[t]['Age 16 to 17'] + d[t]['Age 18 to 19'] + d[t]['Age 20 to 24']\n",
    "d[t]['Age 25 to 44'] = d[t]['Age 25 to 29'] + d[t]['Age 30 to 44']\n",
    "d[t]['Age 45 to 64'] = d[t]['Age 45 to 59'] + d[t]['Age 60 to 64']\n",
    "d[t]['Age 65+'] = d[t]['Age 65 to 74'] + d[t]['Age 75 to 84'] + d[t]['Age 85 to 89'] + d[t]['Age 90 and over']\n",
    "\n",
    "# Select the columns we're interested in analysing\n",
    "selection = ['Age 0 to 4','Age 5 to 14','Age 15 to 24',\n",
    "             'Age 25 to 44','Age 45 to 64','Age 65+']\n",
    "\n",
    "# Create a new data frame to \n",
    "# hold the percentage values\n",
    "# and initialise it with only\n",
    "# the 'mnemonic' (i.e. GeoCode)\n",
    "d_pct = pd.concat(\n",
    "    [d[t]['mnemonic']], \n",
    "    axis=1, \n",
    "    keys=['mnemonic'])\n",
    "\n",
    "# For each of the columns remaining\n",
    "# in the select\n",
    "for c in selection:\n",
    "    d_pct[c] = pd.Series(d[t][c].astype(float)/d[t]['All usual residents'].astype(float))\n",
    "\n",
    "d[t + '_pct'] = d_pct\n",
    "d[t + '_pct'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethnicity\n",
    "\n",
    "We might also think that the balance of ethnic groups might impact a categorisation of LSOAs in London."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'Ethnicity'\n",
    "\n",
    "if np.where(d[t].columns.values=='2011 super output area - lower layer')[0] >= 0:\n",
    "    d[t] = d[t].drop('2011 super output area - lower layer', 1)\n",
    "\n",
    "d[t].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns we're interested in analysing\n",
    "selection = ['White', 'Mixed/multiple ethnic groups', 'Asian/Asian British', \n",
    "             'Black/African/Caribbean/Black British', 'Other ethnic group']\n",
    "\n",
    "# Create a new data frame to \n",
    "# hold the percentage values\n",
    "# and initialise it with only\n",
    "# the 'mnemonic' (i.e. GeoCode)\n",
    "d_pct = pd.concat(\n",
    "    [d[t]['mnemonic']], \n",
    "    axis=1, \n",
    "    keys=['mnemonic'])\n",
    "\n",
    "# For each of the columns remaining\n",
    "# in the select\n",
    "for c in selection:\n",
    "    d_pct[c] = pd.Series(d[t][c].astype(float)/d[t]['All usual residents'].astype(float))\n",
    "\n",
    "d[t + '_pct'] = d_pct\n",
    "d[t + '_pct'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rooms\n",
    "\n",
    "Let's next incorporate the amount of space available to each household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'Rooms'\n",
    "\n",
    "if np.where(d[t].columns.values=='2011 super output area - lower layer')[0] >= 0:\n",
    "    d[t] = d[t].drop('2011 super output area - lower layer', 1)\n",
    "\n",
    "d[t].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns we're interested in analysing\n",
    "selection = ['Does not have central heating', 'Occupancy rating (bedrooms) of -1 or less', \n",
    "             'Average household size', 'Average number of rooms per household']\n",
    "\n",
    "# Create a new data frame to \n",
    "# hold the percentage values\n",
    "# and initialise it with only\n",
    "# the 'mnemonic' (i.e. GeoCode)\n",
    "d_pct = pd.concat(\n",
    "    [d[t]['mnemonic']], \n",
    "    axis=1, \n",
    "    keys=['mnemonic'])\n",
    "\n",
    "# For each of the columns remaining\n",
    "# in the select\n",
    "c = 'Does not have central heating'\n",
    "d_pct[c] = pd.Series(d[t][c].astype(float)/d[t]['All categories: Type of central heating in household'].astype(float))\n",
    "\n",
    "c = 'Occupancy rating (bedrooms) of -1 or less'\n",
    "d_pct[c] = pd.Series(d[t][c].astype(float)/d[t]['All categories: Type of central heating in household'].astype(float))\n",
    "\n",
    "c = 'Average household size'\n",
    "d_pct[c] = pd.Series(d[t][c].astype(float))\n",
    "\n",
    "c = 'Average number of rooms per household'\n",
    "d_pct[c] = pd.Series(d[t][c].astype(float))\n",
    "\n",
    "d[t + '_pct'] = d_pct\n",
    "d[t + '_pct'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles\n",
    "\n",
    "Car ownership and use is also known to be a good predictor of social and economic 'status': Guy Lansley's article on the DLVA's registration database offers a useful perpective on the usefulness of this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'Vehicles'\n",
    "\n",
    "if np.where(d[t].columns.values=='2011 super output area - lower layer')[0] >= 0:\n",
    "    d[t] = d[t].drop('2011 super output area - lower layer', 1)\n",
    "\n",
    "d[t].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns we're interested in analysing\n",
    "selection = [u'No cars or vans in household', u'1 car or van in household',\n",
    "       u'2 cars or vans in household', u'3 cars or vans in household',\n",
    "       u'4 or more cars or vans in household']\n",
    "\n",
    "# Create a new data frame to \n",
    "# hold the percentage values\n",
    "# and initialise it with only\n",
    "# the 'mnemonic' (i.e. GeoCode)\n",
    "d_pct = pd.concat(\n",
    "    [d[t]['mnemonic']], \n",
    "    axis=1, \n",
    "    keys=['mnemonic'])\n",
    "\n",
    "# For each of the columns remaining\n",
    "# in the select\n",
    "for c in selection:\n",
    "    d_pct[c] = pd.Series(d[t][c].astype(float)/d[t]['All categories: Car or van availability'].astype(float))\n",
    "\n",
    "d[t + '_pct'] = d_pct\n",
    "d[t + '_pct'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tenure\n",
    "\n",
    "Ownership structure is another categorisation predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'Tenure'\n",
    "\n",
    "if np.where(d[t].columns.values=='2011 super output area - lower layer')[0] >= 0:\n",
    "    d[t] = d[t].drop('2011 super output area - lower layer', 1)\n",
    "\n",
    "d[t].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns we're interested in analysing\n",
    "selection = [u'Owned', u'Shared ownership (part owned and part rented)', \n",
    "             u'Social rented', u'Private rented', u'Living rent free']\n",
    "\n",
    "# Create a new data frame to \n",
    "# hold the percentage values\n",
    "# and initialise it with only\n",
    "# the 'mnemonic' (i.e. GeoCode)\n",
    "d_pct = pd.concat(\n",
    "    [d[t]['mnemonic']], \n",
    "    axis=1, \n",
    "    keys=['mnemonic'])\n",
    "\n",
    "# For each of the columns remaining\n",
    "# in the select\n",
    "for c in selection:\n",
    "    d_pct[c] = pd.Series(d[t][c].astype(float)/d[t]['All households'].astype(float))\n",
    "\n",
    "d[t + '_pct'] = d_pct\n",
    "d[t + '_pct'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'Qualifications'\n",
    "\n",
    "if np.where(d[t].columns.values=='2011 super output area - lower layer')[0] >= 0:\n",
    "    d[t] = d[t].drop('2011 super output area - lower layer', 1)\n",
    "\n",
    "d[t].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns we're interested in analysing\n",
    "selection = [u'No qualifications',\n",
    "       u'Highest level of qualification: Level 1 qualifications',\n",
    "       u'Highest level of qualification: Level 2 qualifications',\n",
    "       u'Highest level of qualification: Apprenticeship',\n",
    "       u'Highest level of qualification: Level 3 qualifications',\n",
    "       u'Highest level of qualification: Level 4 qualifications and above',\n",
    "       u'Highest level of qualification: Other qualifications']\n",
    "\n",
    "# Create a new data frame to \n",
    "# hold the percentage values\n",
    "# and initialise it with only\n",
    "# the 'mnemonic' (i.e. GeoCode)\n",
    "d_pct = pd.concat(\n",
    "    [d[t]['mnemonic']], \n",
    "    axis=1, \n",
    "    keys=['mnemonic'])\n",
    "\n",
    "# For each of the columns remaining\n",
    "# in the select\n",
    "for c in selection:\n",
    "    d_pct[c] = pd.Series(d[t][c].astype(float)/d[t]['All categories: Highest level of qualification'].astype(float))\n",
    "\n",
    "d[t + '_pct'] = d_pct\n",
    "d[t + '_pct'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'Activity'\n",
    "\n",
    "if np.where(d[t].columns.values=='2011 super output area - lower layer')[0] >= 0:\n",
    "    d[t] = d[t].drop('2011 super output area - lower layer', 1)\n",
    "\n",
    "d[t].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns we're interested in analysing\n",
    "selection = [u'Economically active: In employment',\n",
    "       u'Economically active: Unemployed',\n",
    "       u'Economically active: Full-time student',\n",
    "       u'Economically inactive: Retired',\n",
    "       u'Economically inactive: Looking after home or family',\n",
    "       u'Economically inactive: Long-term sick or disabled',\n",
    "       u'Economically inactive: Other']\n",
    "\n",
    "# Create a new data frame to \n",
    "# hold the percentage values\n",
    "# and initialise it with only\n",
    "# the 'mnemonic' (i.e. GeoCode)\n",
    "d_pct = pd.concat(\n",
    "    [d[t]['mnemonic']], \n",
    "    axis=1, \n",
    "    keys=['mnemonic'])\n",
    "\n",
    "# For each of the columns remaining\n",
    "# in the select\n",
    "for c in selection:\n",
    "    m = re.search(\"^Eco.*?active: (.+)$\", c)\n",
    "    nm = m.group(1)\n",
    "    d_pct[nm] = pd.Series(d[t][c].astype(float)/d[t]['All usual residents aged 16 to 74'].astype(float))\n",
    "\n",
    "d[t + '_pct'] = d_pct\n",
    "d[t + '_pct'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it All Together\n",
    "\n",
    "Now that we've standardised everything using percentages, it's time to bring the data together! We'll initialise the data frame using the first matching data set, and then iterate over the rest, merging the data frames as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = [s for s in d.keys() if \"_pct\" in s]\n",
    "print(matching)\n",
    "\n",
    "lsoac = d[matching[0]]\n",
    "\n",
    "for m in range(1, len(matching)):\n",
    "    lsoac = lsoac.merge(d[matching[m]], how='inner', left_on='mnemonic', right_on='mnemonic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the index\n",
    "lsoac.set_index('mnemonic', drop=True, inplace=True)\n",
    "lsoac.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsoac.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of full data frame is {0} by {1}\".format(lsoac.shape[0], lsoac.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Badly-Behaved Variables\n",
    "\n",
    "Some of these variables will be very difficult to work with because they are so strongly skewed. We can test the degree to which this is a problem fairly easily in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir('outputs') is not True:\n",
    "    print(\"Creating 'outputs' directory for images.\")\n",
    "    os.mkdir('outputs')\n",
    "\n",
    "col_pos=0\n",
    "for c in lsoac.columns.values:\n",
    "    print(\"Creating chart for \" + c)\n",
    "    nm = c.replace(\"/\", \"-\")\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(7,4)\n",
    "    sns.distplot(lsoac[c])\n",
    "    fig.savefig(os.path.join('outputs', \"Untransformed-\" + str(col_pos) + \".\" + nm + '.png'))\n",
    "    plt.close(fig)\n",
    "    col_pos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(lsoac.skew(axis=0, numeric_only=True).values).set_title(\"Skew by Variable for Raw Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = lsoac.skew(axis=0, numeric_only=True)\n",
    "to_drop = sk[sk >= 5].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dropping highly-skewed variables: \" + \", \".join(to_drop.values))\n",
    "lsoac.drop(to_drop.values, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pickle is a 'live' Python class written to \n",
    "# disk -- so it's easy to re-load the data and get \n",
    "# moving again. In other words, if you change your\n",
    "# mind about anything you've done later, you can just\n",
    "# re-start your analysis from the next code block\n",
    "lsoac.to_pickle(os.path.join('data','LSOAC.pickle'))\n",
    "del(lsoac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "The _Geocomputation_ handbook suggests that normalisation via log or Box-Cox transformation happens _after_ the variables have been converted to percentages, so that's what I've done here. I think that this approach is debatable as it's potentially harder to deal with zeroes in the data _after_ converting to a percentage than it was before. The reason that zeroes are an issue is that the log of 0.0 is `-Inf` or `NaN`, so this blows up in your cluster analysis if you don't deal with it now. The easiest way to do this is to simply add `1` to every raw count, ensuring that the smallest value in your data set is always positive. If you had already converted to a percentage then adding 0.000001% to only the zero values still changes the actual distribution, while adding 0.000001% to all values could leave you with percentages over 100!\n",
    "\n",
    "I will try to investigate this further when I have time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoac = pd.read_pickle(os.path.join('data','LSOAC.pickle'))\n",
    "numeric_cols = [col for col in lsoac if lsoac[col].dtype.kind != 'O']\n",
    "lsoac[numeric_cols] += 1\n",
    "print(\"Numeric columns: \" + \", \".join(numeric_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoac['Does not have central heating'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "col_pos = 0\n",
    "for c in lsoac.columns:\n",
    "    if lsoac[c].dtype.kind != 'O':\n",
    "        print(\"Transforming \" + c)\n",
    "        x, _ = boxcox( lsoac[c] )\n",
    "        nm = c.replace(\"/\", \"-\")\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(7,4)\n",
    "        sns.distplot(x, hist=True)\n",
    "        fig.savefig(os.path.join('outputs', \"Box-Cox-\" + str(col_pos) + \".\" + nm + '.png'))\n",
    "        plt.close(fig)\n",
    "        col_pos += 1\n",
    "        lsoac[c] = pd.Series(x, index=lsoac.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Correlated Variables\n",
    "\n",
    "We don't want to keep too many correlated variables in the clustering data since that will bias the clustering algorithms and may result in poor 'performance' in terms of cluster quality (it will be equivalent to some features getting double-weighted!). The best way to do this is to produce a correlation table for all variables and then look to remove problematic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoac.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs    = 0.50 # What's our threshold for strongly correlated?\n",
    "corrh    = 0.70 # What's our threshold for highly correlated?\n",
    "\n",
    "# Generate the matrix but capture the output this time\n",
    "corrm = lsoac.corr()\n",
    "corrm['name'] = corrm.index # We need a copy of the index\n",
    "\n",
    "num_corrs = []\n",
    "hi_corrs  = []\n",
    "\n",
    "for c in corrm.columns:\n",
    "    if c != 'name':\n",
    "        hits = corrm.loc[(abs(corrm[c]) >= corrs) & (abs(corrm[c]) < 1.0), c]\n",
    "        # Some formatting\n",
    "        print(\"=\" * 20 + \" \" + c + \" \" + \"=\" * 20)\n",
    "        \n",
    "        if hits.size == 0: # No correlations > corrs\n",
    "            print(\"Not strongly correlated (>=\" + str(corrs) + \") with other variables.\")\n",
    "        else:\n",
    "            num_corrs.append(hits.size)\n",
    "            \n",
    "            if hits[ abs(hits) <= corrh ].size > 1:\n",
    "                print(\"Strongly correlated with: \")\n",
    "                print(\"\\t\" + \", \".join(hits[ hits <= corrh ].index.values))\n",
    "            \n",
    "            if hits[ abs(hits) > corrh ].size > 1:\n",
    "                print(\"Highly correlated with: \")\n",
    "                print(\"\\t\" + \", \".join(hits[ hits > corrh ].index.values))\n",
    "                hi_corrs.append(hits[ hits > corrh ].size)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(num_corrs, bins=range(1,20), kde=False).set_title(\"Number of Strong (> \" + str(corrs) + \") Correlations with Other Variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(hi_corrs, bins=range(0,20), kde=False).set_title(\"Number of Very Strong Correlations (> \" + str(corrh) + \")  with Other Variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stripping Out 'Redundant' Variables\n",
    "\n",
    "Let's remove any variable that has either:\n",
    "1. A '*lot*' of correlations in excess of 0.50, though we need to define what is 'a lot'.\n",
    "2. A correlation higher than 0.70 with at least one other variable that is already in our 'to keep list.\n",
    "\n",
    "This will reduce the dimensionality of our data and allow us to then focus on normalisation. An alternative approach to dimensionality reduction -- which would be more 'robust', though harder for many to understand -- would be to apply Principal Components Analysis (PCA) to the data set and to work with the eigenvalues afterwards. PCA is available in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcorrs  = 5 # What's our threshold for too many strong correlations?\n",
    "\n",
    "to_drop = [] # Columns to drop\n",
    "to_keep = [] # Columns to keep\n",
    "\n",
    "for c in corrm.columns:\n",
    "    if c != 'name':\n",
    "        \n",
    "        hits = corrm.loc[(abs(corrm[c]) >= corrs) & (abs(corrm[c]) < 1.0), c]\n",
    "        \n",
    "        print(\"=\" * 12 + \" \" + c + \" \" + \"=\" * 12)\n",
    "        print(hits)\n",
    "        print(\" \")\n",
    "        \n",
    "        hi_vals    = False\n",
    "        multi_vals = False\n",
    "        \n",
    "        # Remove ones with very high correlations\n",
    "        if hits[ abs(hits) > corrh ].size > 0:\n",
    "            print(\">>> Very high correlation...\")\n",
    "            s1 = set(to_keep)\n",
    "            s2 = set(hits[ abs(hits) > corrh ].index.values)\n",
    "            #print(\"Comparing to_keep(\" + \", \".join(s1) + \") to hits(\" + \", \".join(s2) + \")\")\n",
    "            s1 &= s2\n",
    "            #print(\"Column found in 'very high correlations': \" + str(s1))\n",
    "            if len(s1) > 1: \n",
    "                hi_vals = True\n",
    "                print(\"Will drop '\" + c + \"' because of very high correlation with retained cols: \\n\\t\" + \"\\n\\t\".join(s1))\n",
    "        \n",
    "        # Remove ones with many correlations\n",
    "        if hits.size >= maxcorrs: \n",
    "            print(\">>> Many correlations...\")\n",
    "            s1 = set(to_keep)\n",
    "            s2 = set(hits.index.values)\n",
    "            #print(\"Comparing to_keep(\" + \", \".join(s1) + \") to hits(\" + \", \".join(s2) + \")\")\n",
    "            s1 &= s2\n",
    "            #print(\"Column found in 'many correlations' :\" + str(s1))\n",
    "            if len(s1) > 1: \n",
    "                multi_vals = True\n",
    "                print(\"Will drop '\" + c + \"' because of multiple strong correlations with retained cols: \\n\\t\" + \"\\n\\t\".join(s1))\n",
    "        \n",
    "        if hi_vals==True or multi_vals==True:\n",
    "            to_drop.append(c)\n",
    "        else:\n",
    "            to_keep.append(c)\n",
    "\n",
    "print(\" \")\n",
    "print(\"To drop: \" + \", \".join(to_drop))\n",
    "print(\" \")\n",
    "print(\"To keep: \" + \", \".join(to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoacf = lsoac.drop(to_drop, axis=1, errors='raise')\n",
    "print(\"Retained variables: \" + \", \".join(lsoacf.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardising the Data\n",
    "\n",
    "The effectiveness of clustering algorithms is usually demonstrated using the 'iris data' -- it's available by default with both Seaborn and SciKit-Learn. This data doesn't usually need normalisation but it's a good way to start looking at the data across four dimensions and seeing how it varies and why some dimensions are 'good' for clustering, while others are 'not useful'..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "irises = sns.load_dataset(\"iris\")\n",
    "sns.pairplot(irises, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Standardise\n",
    "\n",
    "One of the main challenges of clustering, however, is that the scale of each dimension matters: if you were to try to cluster, for example, [1] how many metres per year a glacier moved with [2] the number of cubic metres by which it grew, then you would only be clustering on variable [2]. \n",
    "\n",
    "That's because glaciers contain millions of cubic metres of ice and will grow or shrink by thousands of cubic metres each year. In contrast, most glaciers move at most a few metres per year. So the sheer scale difference between these two dimensions means that the values of variable 1 dominate the clustering algorithm because they provide a much better 'spread' in the data than variable 2.\n",
    "\n",
    "To address this we need to standardise the data in some way so that the scales are relatively consistent. There's no _one_ way to standardise the data, it depends on the characteristics of the data _as well as_ what we're looking for in terms of clustering. As a general rule, we're _aiming_ for a normal (a.k.a. Gaussian) distribution with 0 mean and unit variance. The latter part of this is what most people focus on: you may recall our work with transformations last year, and here's one more reason why it's useful. That said, normalisation (making the data 'look' normal) can also be very important for the process since it can significantly skew the results as well if the data itself is heavily skewed.\n",
    "\n",
    "Right, so does this all sound a little bit familiar from last year? Let's start by just looking at a few variables in a simple scatter plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data as it is now...\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.pairplot(lsoacf, \n",
    "             vars=[\n",
    "                'Asian/Asian British',\n",
    "                'Long-term sick or disabled',\n",
    "                'No qualifications'], \n",
    "             markers=\".\", height=4, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are clearly some differences, but I'd be hard-pressed to give you sensible clusters just by looking at this data.\n",
    "\n",
    "### Standardisation with SKLearn\n",
    "\n",
    "Let's try standardising the data now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how we can rescale in a robust way\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(7,3)\n",
    "sns.distplot(lsoacf['Asian/Asian British'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(\n",
    "    preprocessing.minmax_scale(lsoacf['Asian/Asian British'].values.reshape(-1,1), feature_range=(-1.0, 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full copy, not copy by reference\n",
    "df = lsoacf.copy(deep=True)\n",
    "\n",
    "# An alternative if you'd like to try it\n",
    "#scaler = preprocessing.RobustScaler(quantile_range=[5.0, 95.0])\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "df[to_keep] = scaler.fit_transform(df[to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data as it is now...\n",
    "sns.pairplot(df, \n",
    "             vars=['Asian/Asian British','Long-term sick or disabled','No qualifications'], \n",
    "             markers=\".\", height=4, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so you can see that robustly rescaling the dimension hasn't *actually* changed the relationships within each dimension, or even between dimensions, but it has changed the overall range so that the the data is broadly re-centered on 0 but we *still* have the original outliers from the raw data. You could _also_ do IQR standardisation (0.25 and 0.75) with the percentages, but in those cases you would have _more_ outliers and then _more_ extreme values skewing the results of the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Step Too Far?\n",
    "\n",
    "The standardisation process has given us a better perspective on _where_ high concentrations of different groups might be found, but we still need to decide whether the clustering or other machine learning processes should be influenced by the full range of the data. I followed the approach outlined in _Geocomputation_, but in some ways I lean towards _not_ completely rescaling on the basis that super-high concentrations of particular groups _should_ have a significant impact on the results of the clustering process; however, using robust rescaling (allowing outliers to persist) does mean that we're more likely to get one large cluster containing the bulk of the non-extreme data and a number of small clusters each containing a small number of 'extreme' LSOAs. **Can you think why?**\n",
    "\n",
    "My point is that the right choice is the one that you can argue logically and consistently for. There are plenty of researchers who would disagree with me on the paragraph above, but that doesn't mean I'm wrong. Nor does it mean they're wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Cluster!\n",
    "\n",
    "OK, we're finally here! It's time to cluster the cleaned, normalised, and standardised data set! We're going to start with the best-know clustering technique and work from there...\n",
    "\n",
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#help(KMeans)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few code blocks may take a while to complete, largely because of the `pairplot` at the end where we ask Seaborn to plot every dimension against every other dimension _while_ colouring the points according to their cluster. I've reduced the plotting to just three dimensions, if you want to plot all of them, then just replace the array attached to `vars` with `main_cols`, but you have to bear in mind that that is plotting 4,300 points _each_ time it draws a plot... and there are 81 of them! It'll take a while, but it _will_ do it, and try doing that in Excel or SPSS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check in case something hasn't\n",
    "# run successfully -- these muck up k-means\n",
    "df.drop(list(df.columns[df.isnull().any().values].values), axis=1, inplace=True)\n",
    "\n",
    "k      = 7 # Number of clusters\n",
    "k_var  = 'KMeans' # Variable name\n",
    "kmeans = KMeans(n_clusters=k).fit(df) # The process\n",
    "\n",
    "print(kmeans.labels_) # The results\n",
    "\n",
    "# Add it to the data frame\n",
    "df[k_var] = pd.Series(kmeans.labels_, index=df.index) \n",
    "\n",
    "# How are the clusters distributed?\n",
    "df.KMeans.hist(bins=k)\n",
    "\n",
    "# Going to be a bit hard to read if \n",
    "# we plot every variable against every\n",
    "# other variables, so we'll just pick a \n",
    "# few\n",
    "sns.pairplot(df, \n",
    "             vars=['Age 0 to 4','Asian/Asian British','Long-term sick or disabled'], \n",
    "             hue=k_var, markers=\".\", height=3, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One More Thing...\n",
    "\n",
    "There's just _one_ little problem: what assumption did I make when I started this *k*-means cluster analysis? It's a huge one, and it's one of the reasons that *k*-means clustering _can_ be problematic when used naively...\n",
    "\n",
    "<span style=\"color:red;font-weight:bold;font-style:italic\">**_Discuss the answer to this question with your neighbour._**</span>\n",
    "\n",
    "### The Silhouette Plot vs Within-Cluster Sum of Squares\n",
    "\n",
    "Again, there's more than one way to skin this cat. In _Geocomputation_ they use WCSS to pick the 'optimal' number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (`2...n`) and then look for a 'knee' (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always _some_ benefit to adding more clusters (the perfect clustering is _k==n_), so you don't always see a knee. \n",
    "\n",
    "Another way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the 'quality' of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it's based on Partitioning Around the Medoid (PAM). \n",
    "\n",
    "Either way, to evaluate this in a systematic way, we want to do _multiple_ _k_-means clusterings for _multiple_ values of _k_ and then we can look at which gives the best results...\n",
    "\n",
    "Let's try it for the range 3-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "for k in range(3,10):\n",
    "    # Debugging\n",
    "    print(\"Cluster count: \" + str(k))\n",
    "    \n",
    "    #############\n",
    "    # Do the clustering using the main columns\n",
    "    clusterer = KMeans(n_clusters=k, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(df)\n",
    "    \n",
    "    # Calculate the overall silhouette score\n",
    "    silhouette_avg = silhouette_score(df, cluster_labels)\n",
    "    print(\"For k =\", k,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "    \n",
    "    # Calculate the silhouette values\n",
    "    sample_silhouette_values = silhouette_samples(df, cluster_labels)\n",
    "    \n",
    "    #############\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(9, 5)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1\n",
    "    ax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n",
    "    \n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, df.shape[0] + (k + 1) * 10])\n",
    "    \n",
    "    y_lower = 10\n",
    "    \n",
    "    # For each of the clusters...\n",
    "    for i in range(k):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        # Set the color ramp\n",
    "        cmap = cm.get_cmap(\"Spectral\")\n",
    "        color = cmap(float(i) / k)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed --\n",
    "    # we can only do this for the first two dimensions\n",
    "    # so we may not see fully what is causing the \n",
    "    # resulting assignment\n",
    "    colors = cm.Spectral(cluster_labels.astype(float) / k)\n",
    "    ax2.scatter(df[df.columns[0]], df[df.columns[1]], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    \n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"Visualization of the clustered data\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % k),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "When I ran *k*-means, the results suggested that 5 clusters was probably 'best' -- but note that that's only if we don't have any kind of underlying _theory_, other _empirical evidence_, or just a _reason_ for choosing a different value... Again, we're now getting in areas where _your judgement_ and your ability to _communicate your rationale_ to readers is the key thing. \n",
    "\n",
    "Let's repeat the 5-cluster process and then map it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quick sanity check in case something hasn't\n",
    "# run successfully -- these muck up k-means\n",
    "df.drop(list(df.columns[df.isnull().any().values].values), axis=1, inplace=True)\n",
    "\n",
    "k_pref = 5\n",
    "kmeans = KMeans(n_clusters=k_pref).fit(df)\n",
    "df[k_var] = pd.Series(kmeans.labels_, index=df.index)\n",
    "\n",
    "sdf = gdf.join(df, how='inner')\n",
    "\n",
    "from pysal.contrib.viz import mapping as maps\n",
    "\n",
    "# Where will our shapefile be stored\n",
    "shp_link = os.path.join('data','lsoas.shp')\n",
    "\n",
    "# Save it!\n",
    "sdf.to_file(shp_link)\n",
    "\n",
    "# And now re-load the values from the DBF file \n",
    "# associated with the shapefile.\n",
    "values = np.array(ps.open(shp_link.replace('.shp','.dbf')).by_col(k_var))\n",
    "\n",
    "maps.plot_choropleth(shp_link, values, 'unique_values', \n",
    "                     title='K-Means ' + str(k_pref) + ' Cluster Analysis', \n",
    "                     savein=os.path.join('outputs', 'K-Means.png'), dpi=150, \n",
    "                     figsize=(8,6), alpha=0.9\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sense of whether this is a 'good' result, you might want to visit [datashine](http://datashine.org.uk/#table=QS607EW&col=QS607EW0050&ramp=RdYlGn&layers=BTTT&zoom=10&lon=-0.1751&lat=51.4863) or think back to last year when we examined the NS-SeC data. \n",
    "\n",
    "You could also think of ways of plotting how these groups differ. For instance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kl in range(0,k_pref):\n",
    "    print(\"Cluster \" + str(kl))\n",
    "    lsoas = df[df[k_var]==kl].index\n",
    "    print(\"\\t{0}\".format(len(lsoas)))\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(9,6)\n",
    "    for c in to_keep:\n",
    "        sns.distplot(lsoac[lsoac.index.isin(lsoas.values)][c], hist=False, kde=True, axlabel='')\n",
    "    fig.savefig(os.path.join('outputs', k_var + \"Cluster-\" + str(kl) + '.png'))\n",
    "    plt.close(fig)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Really, really important*\n",
    "\n",
    "Now would be a good time to think about _how_ standardisation and normalisation would have changed your results... and you might want to test whether applying these in a 'stronger' format (e.g. sklearn's `robust_rescale` and scipy's `boxcox`) help or hinder your analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(os.path.join(\"outputs\",\"clusters.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan\n",
    "\n",
    "Of course, as we've said above _k_-means is just one way of clustering, DBScan is another. Unlike _k_-means, we don't need to specify the number of clusters in advance. Which sounds great, but we still need to specify _other_ parameters and these can have a huge impact on our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_var = 'DBSCAN'\n",
    "\n",
    "# Quick sanity check in case something hasn't\n",
    "# run successfully -- these muck up k-means\n",
    "df.drop(list(df.columns[df.isnull().any().values].values), axis=1, inplace=True)\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Run the clustering\n",
    "dbs = DBSCAN(eps=1, min_samples=10).fit(df.values)\n",
    "\n",
    "# See how we did\n",
    "df[d_var] = pd.Series(dbs.labels_, index=df.index)\n",
    "print(df[d_var].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the clustering results we can join it to the Shapefile data (from the geopandas dataframe) and save  it as a new Shapefile containing the new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = gdf.join(df, how='inner')\n",
    "\n",
    "sdf.sample(3)[[k_var,d_var]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create a new visualisation with the output saved to a PNG file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysal.contrib.viz import mapping as maps\n",
    "\n",
    "# Where will our shapefile be stored\n",
    "shp_link = os.path.join('data','lsoas.shp')\n",
    "\n",
    "# Save it!\n",
    "sdf.to_file(shp_link)\n",
    "\n",
    "# And now re-load the values from the DBF file \n",
    "# associated with the shapefile.\n",
    "values = np.array(ps.open(shp_link.replace('.shp','.dbf')).by_col(d_var))\n",
    "\n",
    "maps.plot_choropleth(shp_link, values, 'unique_values', \n",
    "                     title='DBSCAN Cluster Analysis', \n",
    "                     savein=os.path.join('outputs', d_var + '.png'), dpi=150, \n",
    "                     figsize=(8,6), alpha=0.9\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note!\n",
    "\n",
    "Note that you now have the results of the clustering process saved in a shapefile and could then load it into a GIS. Or you could also save the `df` file to a pickle for futher analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(os.path.join(\"outputs\",\"clusters.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Organising Maps\n",
    "\n",
    "SOMs offer a third type of clustering algorithm. They are a relatively 'simple' type of neural network in which the 'map' (of the SOM) adjusts to the data: we're going to see how this works over the next few code blocks, but the main thing is that, unlike the above approaches, SOMs build a 2D map of a higher-dimensional space and use this as a mechanism for subsequently clustering the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Re)Installing SOMPY\n",
    "\n",
    "It would seem that SOMPY is no longer installing correctly by default. Consequently, <span style=\"color:red;font-weight:bold;\">I have left the output from SOMPY in place so that you can see what it will produce *even if you cannot successfully install SOMPY during this practical*</span>. \n",
    "\n",
    "To work out if there is an issue, check to see if the `import` statement below gives you errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sompy.sompy import SOMFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this import has failed with a warning about being unable to find SOM or something similar, then you will need to *re*-install SOMPY using a fork that I created on our Kings GSA GitHub account. For *that* to work, you will need to ensure that you have `git` installed. \n",
    "\n",
    "If the following Terminal command (which should also work in the Windows Terminal) does not give you an error then `git` is already installed:\n",
    "```shell\n",
    "git --version\n",
    "```\n",
    "To install `git` on a Mac is fairly simple. Again, from the Terminal issue the following command:\n",
    "```shell\n",
    "xcode-select --install\n",
    "```\n",
    "This installation may take some time over eduroam since there is a lot to download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that's complete, you can move on to installing SOMPY from our fork. On a Mac this is done on the Terminal with:\n",
    "```shell\n",
    "source activate gsa2018\n",
    "pip install -e git+git://github.com/kingsgeocomp/SOMPY.git#egg=SOMPY\n",
    "source deactivate gsa2018\n",
    "```\n",
    "\n",
    "On Windows you probably drop the `source` and simply say `activate gsa2018`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Get Started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sompy.sompy import SOMFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_var = 'SOM'\n",
    "gdf = gpd.read_file(os.path.join('analysis','lsoas','Lower_Layer_Super_Output_Areas_December_2011_Generalised_Clipped__Boundaries_in_England_and_Wales.shp'))\n",
    "gdf.crs = {'init' :'epsg:27700'}\n",
    "gdf.set_index('lsoa11cd', drop=True, inplace=True)\n",
    "df = pd.read_pickle(os.path.join(\"outputs\",\"clusters.pickle\"))\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where to store the cols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_var  = 'KMeans'\n",
    "d_var  = 'DBSCAN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the inputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = df.drop([k_var,d_var], axis=1).values\n",
    "names = df.columns.values\n",
    "print(data[1:2,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to actually train the SOM using the input data. This is where you specify the input parameters that have the main effect on the clustering results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SOMFactory().build(\n",
    "    data, mapsize=(10,15),\n",
    "    normalization='var', initialization='random', component_names=names)\n",
    "sm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is the fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topographic_error  = sm.calculate_topographic_error()\n",
    "quantization_error = np.mean(sm._bmu[1])\n",
    "print(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the results look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sompy.visualization.mapview import View2D\n",
    "view2D = View2D(10, 10, \"rand data\", text_size=10)\n",
    "view2D.show(sm, col_sz=4, which_dim=\"all\", desnormalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's What I Got\n",
    "\n",
    "<img src=\"https://github.com/kingsgeocomp/applied_gsa/raw/master/img/som_results.png\" alt=\"SOM Clustering Results\" width=\"750\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many clusters do we want and where are they on the map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_val = 5\n",
    "\n",
    "from sompy.visualization.hitmap import HitMapView\n",
    "\n",
    "sm.cluster(k_val)\n",
    "hits  = HitMapView(7, 7, \"Clustering\", text_size=9)\n",
    "a     = hits.show(sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's What I Got\n",
    "\n",
    "<img src=\"https://github.com/kingsgeocomp/applied_gsa/raw/master/img/som_clusters.png\" alt=\"SOM Clustering Results\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many data points were assigned to each BMU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sompy.visualization.bmuhits import BmuHitsView\n",
    "vhts = BmuHitsView(4, 4, \"Hits Map\", text_size=11)\n",
    "vhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's What I Got\n",
    "\n",
    "<img src=\"https://github.com/kingsgeocomp/applied_gsa/raw/master/img/som_heat.png\" alt=\"SOM Heat Map Results\" width=\"750\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's get the cluster results and map them back on to the data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels for each BMU\n",
    "# in the SOM (15 * 10 neurons)\n",
    "clabs = sm.cluster_labels\n",
    "\n",
    "# Project the data on to the SOM\n",
    "# so that we get the BMU for each\n",
    "# of the original data points\n",
    "bmus  = sm.project_data(data)\n",
    "\n",
    "# Turn the BMUs into cluster labels\n",
    "# and append to the data frame\n",
    "df[s_var] = pd.Series(clabs[bmus], index=df.index)\n",
    "\n",
    "print(df.SOM.value_counts())\n",
    "\n",
    "sdf = gdf.join(df, how='inner')\n",
    "\n",
    "sdf.sample(5)[[k_var,d_var,s_var]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysal.contrib.viz import mapping as maps\n",
    "\n",
    "# Where will our shapefile be stored\n",
    "shp_link = os.path.join('data','lsoas.shp')\n",
    "\n",
    "# Save it!\n",
    "sdf.to_file(shp_link)\n",
    "\n",
    "# And now re-load the values from the DBF file \n",
    "# associated with the shapefile.\n",
    "values = np.array(ps.open(shp_link.replace('.shp','.dbf')).by_col(s_var))\n",
    "\n",
    "maps.plot_choropleth(shp_link, values, 'unique_values', \n",
    "                     title='SOM Cluster Analysis', \n",
    "                     savein=os.path.join('outputs', s_var + '.png'), dpi=150, \n",
    "                     figsize=(8,6), alpha=0.9\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's What I Got\n",
    "\n",
    "<img src=\"https://github.com/kingsgeocomp/applied_gsa/raw/master/img/som_cluster_map.png\" alt=\"SOM Clustering Results Mapped\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "\n",
    "You've reached the end, you're done... \n",
    "\n",
    "Er, no. This is barely scratching the surface! I'd suggest that you go back through the above code and do three things:\n",
    "1. Add a lot more comments to the code to ensure that really have understood what is going on.\n",
    "2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change.\n",
    "3. Try outputting additional plots that will help you to understand the _quality_ of your clustering results (e.g. what _is_ the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\n",
    "\n",
    "If all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\n",
    "\n",
    "See: [Introduction to Machine Learning with Scikit-Learn](http://www.slideshare.net/BenjaminBengfort/introduction-to-machine-learning-with-scikitlearn)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Geocomp",
   "language": "python",
   "name": "gsa2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
